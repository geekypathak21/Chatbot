{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to understand chatbot.py\n",
    "\n",
    "This notebook contains summary how seq2seq model is applied in this chatbot. It contains explaination of whole code how our chatbot.py code is working.\n",
    "So first we import all the required packages in chatbot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re \n",
    "import time\n",
    "from model import seq2seq_model,model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here model package contain the implementaion of seq2seq model using tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step is to import our dataset \n",
    "Dataset used in this chatbot is cornell movie corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore' ).read().split('\\n')\n",
    "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore' ).read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a dictionary that maps each line and it's id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should create a list of all conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_ids = []\n",
    "for conversation in conversations[:-1]:\n",
    "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\" , \"\").replace(\" \",\"\")\n",
    "    conversation_ids.append(_conversation.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperating questions and answers from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "for conversation in conversation_ids:\n",
    "    for i in range(len(conversation)-1):\n",
    "        questions.append(id2line[conversation[i]])\n",
    "        answers.append(id2line[conversation[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a first cleaning of the texts\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\",\"i am\",text)\n",
    "    text = re.sub(r\"he's\",\"he is\",text)\n",
    "    text = re.sub(r\"that's\",\"he is\",text)\n",
    "    text = re.sub(r\"she's\",\"she is\",text)\n",
    "    text = re.sub(r\"what's\",\"what is\",text)\n",
    "    text = re.sub(r\"it's\",\"it is\",text)\n",
    "    text = re.sub(r\"\\'ll\",\"will\",text)\n",
    "    text = re.sub(r\"we'd\",\"we had\",text)\n",
    "    text = re.sub(r\"we're\",\"we were\",text)\n",
    "    text = re.sub(r\"can't\",\"can not\",text)\n",
    "    text = re.sub(r\"would'nt\",\"would not\",text)\n",
    "    text = re.sub(r\"should'nt\",\"should not\",text)\n",
    "    text = re.sub(r\"they're\",\"they were\",text)\n",
    "    text = re.sub(r\"they'd\",\"they had\",text)\n",
    "    text = re.sub(r\"\\'re\",\"are\",text)\n",
    "    text = re.sub(r\"\\'d\",\"had\",text)\n",
    "    text = re.sub(r\"\\'ve\",\"have\",text)\n",
    "    text = re.sub(r\"[-()#/@;:<>{}+=~|.?,]\",\"\",text)\n",
    "    return text\n",
    "#Cleaning the questions \n",
    "clean_question = []\n",
    "for question in questions:\n",
    "    question = clean_text(question)\n",
    "    clean_question.append(question)\n",
    "#Cleaning the answers\n",
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))\n",
    "#Creating a dictionary that maps each word to its number of occurences\n",
    "words2count = {}\n",
    "for question in clean_question:\n",
    "    for word in question.split():\n",
    "        if word not in words2count:\n",
    "            words2count[word] = 1\n",
    "        else:\n",
    "            words2count[word]+=1\n",
    "for answer in clean_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in words2count:\n",
    "            words2count[word] = 1\n",
    "        else:\n",
    "            words2count[word]+=1     \n",
    "#Creating two dictionaries that maps question words and the answer words to a unique integer\n",
    "threshold_questions = 20\n",
    "questionwords2int = {}\n",
    "word_number = 0\n",
    "for word,count in words2count.items():\n",
    "    if count>=threshold_questions:\n",
    "        questionwords2int[word] = word_number\n",
    "        word_number+=1\n",
    "threshold_answers = 20        \n",
    "answerwords2int = {}\n",
    "word_number = 0\n",
    "for word,count in words2count.items():\n",
    "    if count>=threshold_answers:\n",
    "        answerwords2int[word] = word_number\n",
    "        word_number+=1\n",
    "\n",
    "#Adding the last tokens to these dictionaries\n",
    "tokens =['<PAD>','<EOS>','<OUT>','<SOS>']\n",
    "for token in tokens:\n",
    "    questionwords2int[token]=len(questionwords2int)+1\n",
    "for token in tokens:\n",
    "    answerwords2int[token]=len(answerwords2int)+1            \n",
    "#Creating the inverse dictionary of the answerwords2int\n",
    "answerints2word={w_i:w for w,w_i in answerwords2int.items()}\n",
    "#Adding the end of string token to the end of every answer    \n",
    "for i in range(len(clean_answers)):\n",
    "    clean_answers[i]+='<EOS>'\n",
    "\"\"\" Translating all the questions and answers in to integers and replacing \n",
    "     all the words that were filtered out by<OUT>\"\"\"\n",
    "question_to_int = []\n",
    "for question in clean_question:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questionwords2int:\n",
    "            ints.append(questionwords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(questionwords2int[word])\n",
    "    question_to_int.append(ints)\n",
    "\n",
    "answer_to_int = []\n",
    "for answer in clean_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answerwords2int:\n",
    "            ints.append(answerwords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(answerwords2int[word])\n",
    "    answer_to_int.append(ints)\n",
    "#Sorting questions and answers by the length of question\n",
    "sorted_clean_questions = []\n",
    "sorted_clean_answers = []\n",
    "for length in range(1,25+1):\n",
    "    for i in enumerate(question_to_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_clean_questions.append(question_to_int[i[0]])\n",
    "            sorted_clean_answers.append(answer_to_int[i[0]])\n",
    "            \n",
    "#Training The SEQ2SEQ model\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 512\n",
    "num_layers = 3\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.5\n",
    "\n",
    "#Defining a session\n",
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "#Loading the model inputs\n",
    "inputs, targets, lr, keep_prob = model_inputs()\n",
    "\n",
    "#Setting the sequence length\n",
    "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')\n",
    "\n",
    "#Getting the shape of input tensor\n",
    "input_shape = tf.shape(inputs)\n",
    "\n",
    "#Getting the training and test predictions\n",
    "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                       targets,\n",
    "                                                       keep_prob,\n",
    "                                                       batch_size,\n",
    "                                                       sequence_length,\n",
    "                                                       len(answerwords2int),\n",
    "                                                       len(questionwords2int),\n",
    "                                                       encoding_embedding_size,\n",
    "                                                       decoding_embedding_size,\n",
    "                                                       rnn_size,\n",
    "                                                       num_layers,\n",
    "                                                       questionwords2int)\n",
    "#Setting up the loss error,the optimizer gradient clipping\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    loss_error = tf.contrib.seq2seq.Sequence_loss(training_predictions,targets,tf.ones([input_shape[0],sequence_length]))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss_error)\n",
    "    clipped_gradients = [(tf.clip_by_value(grad_tensor,-5.,5.),grad_variable) for grad_tensor,grad_variable in gradients if grad_tensor is not None]\n",
    "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)\n",
    "#Padding the sequence with the <PAD> token\n",
    "def apply_padding(batch_of_sequences, word2int):\n",
    "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences]) \n",
    "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]\n",
    "#Splitting the data into batches of questions and answers\n",
    "def split_into_batches(questions, answers, batchsize):\n",
    "    for batch_index in range (0, len(questions)//batch_size):\n",
    "        start_index = batch_index*batch_size\n",
    "        question_in_batch = questions[ start_index : start_index+batch_size]\n",
    "        answer_in_batch = answers[start_index : start_index+batch_size]\n",
    "        padded_question_in_batch = np.array(apply_padding(question_in_batch, questionwords2int))\n",
    "        padded_answers_in_batch = np.array(apply_padding(answer_in_batch, answerwords2int))\n",
    "        yield padded_question_in_batch, padded_answers_in_batch \n",
    "#Splitting the question and answers into training and validation sets\n",
    "training_validation_split = int(len(sorted_clean_questions)*0.15)\n",
    "training_questions = sorted_clean_questions[training_validation_split:]\n",
    "training_answers = sorted_clean_answers[training_validation_split:]\n",
    "validation_questions = sorted_clean_questions[:training_validation_split]\n",
    "validation_answer = sorted_clean_answers[:training_validation_split] \n",
    "\n",
    "#Training the model\n",
    "batch_index_check_training_loss = 100\n",
    "batch_index_check_validation_loss = ((len(training_questions))//batch_size//2)-1\n",
    "total_training_loss_error = 0\n",
    "list_validation_loss_error = []\n",
    "early_stopping_check = 0\n",
    "early_stopping_stop  = 1000\n",
    "checkpoint = \"chatbot_weights.ckpt\"\n",
    "session.run(tf.global_variables_initializer())\n",
    "for epochs in range(1, epochs+1):\n",
    "    for batch_index, (padded_question_in_batch, padded_answer_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
    "        starting_time = time.time()\n",
    "        _,batch_training_loss_error = session.run([optimizer_gradient_clipping,loss_error],{inputs:padded_question_in_batch,targets:padded_answer_in_batch,lr:learning_rate,sequence_length:padded_answer_in_batch.shape[1],keep_prob:keep_probability})\n",
    "        total_training_loss_error += batch_training_loss_error\n",
    "        ending_time = time.time()\n",
    "        batch_time = ending_time - starting_time\n",
    "        if batch_index % batch_index_check_training_loss == 0:\n",
    "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epochs,\n",
    "                                                                                                                                       epochs,\n",
    "                                                                                                                                       batch_index,\n",
    "                                                                                                                                       len(training_questions) // batch_size,\n",
    "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
    "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
    "            total_training_loss_error = 0\n",
    "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
    "            total_validation_loss_error = 0\n",
    "            starting_time = time.time()\n",
    "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answer, batch_size)):\n",
    "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
    "                                                                       targets: padded_answers_in_batch,\n",
    "                                                                       lr: learning_rate,\n",
    "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                       keep_prob: 1})\n",
    "                total_validation_loss_error += batch_validation_loss_error\n",
    "            ending_time = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
    "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            list_validation_loss_error.append(average_validation_loss_error)\n",
    "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                print('I speak better now!!')\n",
    "                early_stopping_check = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, checkpoint)\n",
    "            else:\n",
    "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
    "                early_stopping_check += 1\n",
    "                if early_stopping_check == early_stopping_stop:\n",
    "                    break\n",
    "    if early_stopping_check == early_stopping_stop:\n",
    "        print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
    "        break\n",
    "print(\"Game Over\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
